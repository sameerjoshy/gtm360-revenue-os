export const insights = {
    "more-pipeline-doesnt-fix-growth": {
        slug: "/insights/more-pipeline-doesnt-fix-growth",
        title: "Why More Pipeline Doesn’t Fix B2B Growth",
        category: "Pipeline & Deals",
        readTime: "6 min read",
        description: "Why adding 3x coverage to a broken system just accelerates burnout, not revenue.",
        relatedService: { name: "Pipeline & Deal Quality Repair", link: "/services/pipeline-quality" },
        sections: {
            problemRecognition: `
                <p>The situation is painfully common in B2B reviews:</p>
                <p>The team hits their activity targets. Marketing delivers the MQL commit. The CRM shows "3x coverage" for the quarter.</p>
                <p>Yet, by week 8 of the quarter, the forecast starts slipping. Deals that looked "committed" get pushed. The gap to plan widens, and the scramble begins.</p>
                <p>Leadership looks at the data and sees a math problem: <em>"If we closed 20% of $10M last time, we just need $15M in pipe to hit our growth number."</em></p>
            `,
            wrongExplanation: `
                <p>The default diagnosis is almost always volume.</p>
                <ul>
                    <li>"We need more at bats."</li>
                    <li>"Marketing needs to fill the top of the funnel."</li>
                    <li>"SDRs aren't making enough calls."</li>
                </ul>
                <p>It is a reasonable assumption. In simple systems, output is directly correlated to input. If you want more manufacturing output, you add more raw materials. Sales leaders, under pressure from boards who love "coverage metrics," double down on generation.</p>
            `,
            whyItFails: `
                <p>This explanation fails because B2B sales is not a manufacturing line; it is a filtration system.</p>
                <p>When you dump more volume into a system with unclear "exit criteria" (how a deal moves from one stage to the next), you don't get more revenue. You get more noise.</p>
                <p><strong>Volume breaks valid signals.</strong> When reps have 50 opps instead of 15, they cannot vigorously disqualify. They keep "maybe" deals alive to show activity.</p>
                <p><strong>Bad deals consume the resources meant for good deals.</strong> Every hour spent prepping a demo for a curious non-buyer is an hour stolen from a real champion.</p>
            `,
            actualConstraint: `
                <p>The constraint is not <strong>Entry Volume</strong>. It is <strong>Throughput Integrity</strong>.</p>
                <p>Specifically, the definitions of "Qualified" and "Stage Progression" are usually based on <em>seller activity</em> (we gave a demo) rather than <em>buyer verification</em> (they agreed to a pilot).</p>
                <p>If your Stage 2 -> Stage 3 conversion rate is low, adding 2x more leads to Stage 1 implies you will just waste 2x more SDR time—not close 2x more deals.</p>
            `,
            consequences: `
                <p>Ignoring this creates a "Death Spiral of Coverage":</p>
                <ol>
                    <li>You demand more pipeline.</li>
                    <li>Marketing lowers the bar to hit the lead number.</li>
                    <li>Sales gets flooded with low-intent conversations.</li>
                    <li>Win rates drop because focus is diluted.</li>
                    <li>You miss the number.</li>
                    <li>You conclude you need <em>even more pipeline</em> to make up for the lower win rate.</li>
                </ol>
                <p>This burns cash, burns out reps, and confuses product teams who get feedback from people who were never going to buy.</p>
            `,
            whatChanges: `
                <p>To fix this, you must stop solving for coverage and start solving for transfer.</p>
                <ul>
                    <li><strong>Redefine "Qualified":</strong> It is not "BANT." It is a verified exchange of value.</li>
                    <li><strong>Hard Gating:</strong> Reps cannot move a deal to Stage 2 without an artifact from the buyer (e.g., a mutual success plan or access to power).</li>
                    <li><strong>Kill "Zombie" Deals:</strong> If a deal hasn't moved in 30 days, it is closed-lost. No exceptions.</li>
                </ul>
                <p>Buying more leads won't help until you fix the filter.</p>
            `,
            ourView: `
                <p>We do not believe in "3x coverage" as a universal law.</p>
                <p>We believe in high-velocity disqualification. The healthiest pipelines often look "light" at the start of the quarter because they contain only real opportunities.</p>
                <p>Revenue is generated by closing, not by opening.</p>
            `
        }
    },
    // Scaffolds for other 5 (Content to be filled, structure ready)
    "win-rate-is-not-a-sales-problem": {
        slug: "/insights/win-rate-is-not-a-sales-problem",
        title: "Your Win Rate Isn’t a Sales Problem",
        category: "Pipeline & Deals",
        readTime: "5 min read",
        description: "Deals don't die because reps forgot how to sell. They die because the system sent them to the wrong battle.",
        relatedService: { name: "Pipeline & Deal Quality Repair", link: "/services/pipeline-quality" },
        sections: {
            problemRecognition: `
                <p>Win rates are the most scrutinized metric in B2B sales, yet the most misunderstood.</p>
                <p>When win rates drop, the reaction is almost pavlovian: <strong>Train the reps.</strong></p>
                <ul>
                    <li>"We need better objection handling."</li>
                    <li>"We need more negotiation training."</li>
                    <li>"We need to be more aggressive closers."</li>
                </ul>
                <p>Sales enablement is deployed. Methodologies like MEDDIC or Challenger are rolled out. But 6 months later, the win rate is exactly the same.</p>
            `,
            wrongExplanation: `
                <p>The failure comes from assuming that <em>"Closed Lost"</em> means <em>"Failed to Sell."</em></p>
                <p>In mature markets, this is rarely true.</p>
                <p>Most deals are not lost because the rep stumbled over a feature request or missed a closing trick. They are lost because <strong>the deal was structurally unwinnable from the start.</strong></p>
                <p>If you send a Navy SEAL into a battle they are outnumbered 100:1, they will lose. That is not a "soldier problem." That is a "general problem."</p>
            `,
            whyItFails: `
                <p>Training reps to "sell better" on bad opportunities is counter-productive.</p>
                <p>It encourages them to spend <em>more</em> energy on deals that should have been disqualified in minute 5.</p>
                <p>When you pressure reps to "improve win rates" without fixing the upstream filters, you effectively tell them: <em>"Never give up on anything."</em> This clogs the pipeline with zombie deals that suck oxygen from the few real opportunities that could actually close.</p>
            `,
            actualConstraint: `
                <p>The constraint is <strong>Single-Threaded Selling</strong>.</p>
                <p>Most lost deals aren't "lost" to a competitor. They are lost to <strong>Indecision</strong>.</p>
                <p>This happens because the rep spent 3 months selling to a "Champion" who lacked the political capital to get the budget approved. We won the argument with the user, but we lost the vote in the boardroom.</p>
            `,
            consequences: `
                <p>When you focus on "Closing Skills" instead of "Consensus Building":</p>
                <ol>
                    <li>Reps get "happy ears" from friendly champions.</li>
                    <li>Forecasts are fictional because they depend on one person's optimism.</li>
                    <li>Deals slip indefinitely because no one knows who actually holds the pen.</li>
                </ol>
            `,
            whatChanges: `
                <p>To actually close more deals, you must stop "selling" and start "orchestrating."</p>
                <ul>
                    <li><strong>Multithreading is Mandatory:</strong> You cannot be in Stage 3 with only one contact. You must access Power, Finance, and Technical Validation independently.</li>
                    <li><strong>Sell the Problem, Not the Product:</strong> The CFO doesn't care about your features. They care about the risk of *doing nothing*. Build a business case that survives without you in the room.</li>
                    <li><strong>Mutual Action Plans:</strong> Don't "hope" for a close date. Engineering the sequence of approval steps backwards from the go-live date.</li>
                </ul>
            `,
            ourView: `
                <p>Closing isn't a magic phrase you say at the end of the meeting.</p>
                <p>Closing is the result of aligning the entire buying committee around a shared problem.</p>
                <p>If you have to "hard close," you missed a step two months ago.</p>
            `
        }
    },
    "forecasting-fails-with-good-data": {
        slug: "/insights/forecasting-fails-with-good-data",
        title: "Why Forecasting Fails Even With Good Data",
        category: "Forecasting",
        readTime: "5 min read",
        description: "Why 'commit' means nothing if the underlying signal logic is based on opinion.",
        relatedService: { name: "Forecasting & Revenue Governance", link: "/services/forecasting-governance" },
        sections: {
            problemRecognition: "<p>Coming soon...</p>",
            wrongExplanation: "<p>Coming soon...</p>",
            whyItFails: "<p>Coming soon...</p>",
            actualConstraint: "<p>Coming soon...</p>",
            consequences: "<p>Coming soon...</p>",
            whatChanges: "<p>Coming soon...</p>",
            ourView: "<p>Coming soon...</p>"
        }
    },
    "messaging-fails-in-real-deals": {
        slug: "/insights/messaging-fails-in-real-deals",
        title: "When Messaging Sounds Right but Doesn’t Move Deals",
        category: "Positioning & ICP",
        readTime: "6 min read",
        description: "Marketing copy vs. Sales reality. Why the 'perfect pitch' falls flat in the boardroom.",
        relatedService: { name: "GTM Operating Model Realignment", link: "/services/gtm-operating-model" },
        sections: {
            problemRecognition: "<p>Coming soon...</p>",
            wrongExplanation: "<p>Coming soon...</p>",
            whyItFails: "<p>Coming soon...</p>",
            actualConstraint: "<p>Coming soon...</p>",
            consequences: "<p>Coming soon...</p>",
            whatChanges: "<p>Coming soon...</p>",
            ourView: "<p>Coming soon...</p>"
        }
    },
    "revops-dashboards-dont-drive-decisions": {
        slug: "/insights/revops-dashboards-dont-drive-decisions",
        title: "Why RevOps Dashboards Don’t Drive Decisions",
        category: "RevOps & Signals",
        readTime: "5 min read",
        description: "We are swimming in data but starving for insight. The difference between reporting and steering.",
        relatedService: { name: "Forecasting & Revenue Governance", link: "/services/forecasting-governance" },
        sections: {
            problemRecognition: "<p>Coming soon...</p>",
            wrongExplanation: "<p>Coming soon...</p>",
            whyItFails: "<p>Coming soon...</p>",
            actualConstraint: "<p>Coming soon...</p>",
            consequences: "<p>Coming soon...</p>",
            whatChanges: "<p>Coming soon...</p>",
            ourView: "<p>Coming soon...</p>"
        }
    },
    "ai-amplifies-bad-gtm-models": {
        slug: "/insights/ai-amplifies-bad-gtm-models",
        title: "AI Doesn’t Fix GTM Problems — It Amplifies Them",
        category: "Tools & AI",
        readTime: "7 min read",
        description: "Automating a chaotic process just creates chaos faster. The danger of AI without system clarity.",
        relatedService: { name: "GTM Signals, Tooling & AI Alignment", link: "/services/gtm-signals-and-ai" },
        sections: {
            problemRecognition: "<p>Coming soon...</p>",
            wrongExplanation: "<p>Coming soon...</p>",
            whyItFails: "<p>Coming soon...</p>",
            actualConstraint: "<p>Coming soon...</p>",
            consequences: "<p>Coming soon...</p>",
            whatChanges: "<p>Coming soon...</p>",
            ourView: "<p>Coming soon...</p>"
        }
    }
};

export const caseStudies = {
    "fixing-the-wrong-problem": {
        slug: "/insights/case-studies/fixing-the-wrong-problem",
        title: "When Fixing the “Obvious” Problem Made Growth Worse",
        subtitle: "A real-world example of how execution pressure compounded failure — until the underlying GTM misdiagnosis was corrected.",
        thesis: "Execution pressure compounded failure until the underlying GTM misdiagnosis was corrected.",
        description: "A diagnostic case study showing how fixing the wrong GTM problem compounded failure — and what changed once the real constraint was identified.",
        category: "Diagnostic Case Study",
        readTime: "8 min read",
        sections: {
            context: `
                <p>This case involved a B2B company with a mature sales organization, a functioning pipeline, and steady inbound demand.</p>
                <p>On the surface, the situation looked familiar:</p>
                <ul>
                    <li>growth had slowed</li>
                    <li>leadership felt execution quality had dropped</li>
                    <li>teams were under pressure to “do more”</li>
                </ul>
                <p>There was no shortage of effort, tooling, or activity.</p>
                <p>What was missing was clarity.</p>
            `,
            initialBelief: `
                <p>Leadership believed the problem was execution drift.</p>
                <p>Specifically:</p>
                <ul>
                    <li>sales teams were not converting enough opportunities</li>
                    <li>pipeline coverage looked sufficient, but outcomes lagged</li>
                    <li>deal velocity felt slower than it should be</li>
                </ul>
                <p>The prevailing assumption was simple:</p>
                <p><em>“We need tighter sales execution and more pipeline focus.”</em></p>
                <p>This belief drove the next set of actions.</p>
            `,
            whyReasonable: `
                <p>The misdiagnosis made sense at the time.</p>
                <ul>
                    <li>CRM dashboards showed pipeline volume</li>
                    <li>Sales activity levels were high</li>
                    <li>Previous growth phases had responded well to execution pressure</li>
                    <li>Leadership had solved similar slowdowns this way before</li>
                </ul>
                <p>Nothing looked obviously broken — which made the issue harder to identify.</p>
            `,
            diagnosticReveal: `
                <p>The diagnostic surfaced a different constraint entirely.</p>
                <p>The core issue was not sales execution.</p>
                <p>It was a system-level misalignment between:</p>
                <ul>
                    <li>who the company believed it was selling to</li>
                    <li>how deals were being qualified</li>
                    <li>and how buyers were actually making decisions</li>
                </ul>
                <p>As a result:</p>
                <ul>
                    <li>“qualified” opportunities entered the pipeline too early</li>
                    <li>late-stage deals stalled due to unresolved buyer ownership</li>
                    <li>sales effort increased in areas that could not convert</li>
                </ul>
                <p>The system was functioning exactly as designed.</p>
                <p>The design itself was the problem.</p>
            `,
            whatChanged: `
                <p>Once the real constraint was understood, the focus shifted.</p>
                <p>Instead of pushing harder on execution:</p>
                <ul>
                    <li>qualification logic was redefined around buyer ownership</li>
                    <li>deal stages were aligned to real decision points</li>
                    <li>teams were encouraged to exit deals earlier, not carry them longer</li>
                    <li>leadership stopped measuring success by activity volume alone</li>
                </ul>
                <p>Several initiatives were deliberately stopped.</p>
                <p>More effort was not the answer — better sequencing was.</p>
            `,
            outcome: `
                <p>The first visible change was not growth metrics.</p>
                <p>It was decision clarity.</p>
                <ul>
                    <li>sales conversations became more consistent</li>
                    <li>late-stage surprises reduced</li>
                    <li>leadership regained confidence in what the pipeline represented</li>
                    <li>teams spent less time chasing progress and more time qualifying reality</li>
                </ul>
                <p>Growth followed later — but only after the system was corrected.</p>
            `,
            whyMatters: `
                <p>This pattern appears frequently in B2B GTM systems.</p>
                <p>When execution pressure increases without revisiting diagnosis:</p>
                <ul>
                    <li>effort compounds the wrong behavior</li>
                    <li>tools amplify confusion</li>
                    <li>teams burn energy without learning</li>
                </ul>
                <p>Fixing the wrong problem well still produces bad outcomes.</p>
            `
        },
        relatedLinks: [
            { text: "System Component: Pipeline Quality Repair", url: "/services/pipeline-quality" },
            { text: "Common Symptoms: See the 8 patterns", url: "/problems" }
        ]
    },
    "when-ai-created-noise-not-clarity": {
        slug: "/insights/case-studies/when-ai-created-noise-not-clarity",
        title: "When AI Increased Activity but Reduced GTM Clarity",
        subtitle: "A real example of how automation amplified confusion — until the underlying decision system was corrected.",
        thesis: "Automation amplified noise because decision logic was never defined.",
        description: "A diagnostic case study showing how AI and dashboards amplified GTM confusion instead of improving decisions — and what corrected it.",
        category: "Diagnostic Case Study",
        readTime: "7 min read",
        sections: {
            context: `
                <p>This case involved a B2B company that had invested heavily in RevOps tooling and AI-assisted analytics.</p>
                <p>Leadership believed visibility was the problem:</p>
                <ul>
                    <li>more dashboards were built</li>
                    <li>more signals were tracked</li>
                    <li>more automation was introduced</li>
                </ul>
                <p>On paper, the GTM system looked increasingly sophisticated.</p>
                <p>In practice, decision-making deteriorated.</p>
            `,
            initialBelief: `
                <p>The organization believed the issue was insufficient insight.</p>
                <p>Specifically:</p>
                <ul>
                    <li>leaders felt they lacked real-time visibility</li>
                    <li>frontline teams wanted clearer prioritization</li>
                    <li>executives believed better data would improve judgment</li>
                </ul>
                <p>The assumption was straightforward:</p>
                <p><em>“If we can see more, we can decide better.”</em></p>
                <p>This belief drove a rapid expansion of tools and AI usage.</p>
            `,
            whyReasonable: `
                <p>The logic appeared sound.</p>
                <ul>
                    <li>dashboards were technically accurate</li>
                    <li>AI surfaced large volumes of signals</li>
                    <li>activity metrics increased across the board</li>
                    <li>reporting cadence improved</li>
                </ul>
                <p>From a tooling perspective, the system looked mature.</p>
                <p>But clarity did not improve.</p>
            `,
            diagnosticReveal: `
                <p>The diagnostic uncovered a deeper issue.</p>
                <p>The problem was not lack of insight.</p>
                <p>It was the absence of a decision model.</p>
                <p>Specifically:</p>
                <ul>
                    <li>signals were not tied to decisions</li>
                    <li>metrics existed without consequence</li>
                    <li>AI amplified data that had no governing logic</li>
                </ul>
                <p>As a result:</p>
                <ul>
                    <li>teams reacted to noise</li>
                    <li>leaders debated dashboards instead of decisions</li>
                    <li>automation accelerated misalignment</li>
                </ul>
                <p>AI was functioning correctly.</p>
                <p>The system it was amplifying was not.</p>
            `,
            whatChanged: `
                <p>The corrective action was not adding better AI.</p>
                <p>Instead:</p>
                <ul>
                    <li>signals were explicitly mapped to decisions</li>
                    <li>most dashboards were retired</li>
                    <li>leadership defined which metrics mattered — and why</li>
                    <li>AI usage was constrained to amplify only validated signals</li>
                </ul>
                <p>Less data produced better judgment.</p>
            `,
            outcome: `
                <p>The immediate improvement was not efficiency.</p>
                <p>It was control.</p>
                <ul>
                    <li>leadership conversations became focused</li>
                    <li>teams understood what signals required action</li>
                    <li>automation supported judgment instead of replacing it</li>
                    <li>confidence returned — without adding tools</li>
                </ul>
                <p>Only after this correction did AI begin to add leverage.</p>
            `,
            whyMatters: `
                <p>This pattern is increasingly common.</p>
                <p>AI does not fix GTM systems.<br/>It magnifies them.</p>
                <p>When the underlying decision logic is weak,<br/>automation compounds confusion at scale.</p>
            `
        },
        relatedLinks: [
            { text: "System Component: GTM Signals & AI", url: "/services/gtm-signals-and-ai" },
            { text: "Common Symptoms: See the 8 patterns", url: "/problems" }
        ]
    }
};
